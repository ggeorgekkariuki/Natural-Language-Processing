{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP with RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMuG6LHQ1MzUdA8ACopClHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ggeorgekkariuki/Natural-Language-Processing/blob/main/NLP_with_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUO68aVHlaR1",
        "outputId": "f55912a1-a1ec-4a71-bf18-b5606df5d6f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "`%tensorflow_version` only switches the major version: 1.x or 2.x.\n",
            "You set: `2.x  # this line is not required unless you are in a notebook`. This will be interpreted as: `2.x`.\n",
            "\n",
            "\n",
            "TensorFlow 2.x selected.\n"
          ]
        }
      ],
      "source": [
        "%tensorflow_version 2.x  # this line is not required unless you are in a notebook\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing import sequence\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import numpy as np\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "VOCAB_SIZE = 88584\n",
        "\n",
        "MAXLEN = 250\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bkEfbWFlpTZ",
        "outputId": "7535a35a-5dfe-4c8c-8ac9-2d0612f2d730"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n",
            "17473536/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###More Preprocessing\n",
        "If we have a look at some of our loaded in reviews, we'll notice that they are different lengths. This is an issue. We cannot pass different length data into our neural network. Therefore, we must make each review the same length. To do this we will follow the procedure below:\n",
        "- if the review is greater than 250 words then trim off the extra words\n",
        "- if the review is less than 250 words add the necessary amount of 0's to make it equal to 250."
      ],
      "metadata": {
        "id": "uktIn9XolxZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = sequence.pad_sequences(train_data, MAXLEN)\n",
        "test_data = sequence.pad_sequences(test_data, MAXLEN)"
      ],
      "metadata": {
        "id": "E_Bdg_aPmRlE"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAuM-h37m7yg",
        "outputId": "ca40d9d8-606e-4081-b005-72345f61295b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(250,)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 250)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Creating the Model\n",
        "Now it's time to create the model. \n",
        "\n",
        "We'll use a word embedding layer as the first layer in our model and add a LongShortTermMemory layer afterwards that feeds into a dense node to get our predicted sentiment. \n",
        "\n",
        "32 stands for the output dimension of the vectors generated by the embedding layer. "
      ],
      "metadata": {
        "id": "b82x4L-MnS2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential(\n",
        "    [\n",
        "     tf.keras.layers.Embedding(VOCAB_SIZE, 32),\n",
        "     tf.keras.layers.LSTM(32),\n",
        "     tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ]\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf8FcQhenCZH",
        "outputId": "b1575a3a-23fb-4203-e96f-c7cdcc644450"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, None, 32)          2834688   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,843,041\n",
            "Trainable params: 2,843,041\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Training"
      ],
      "metadata": {
        "id": "SAv79K8kpVAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='binary_crossentropy',\n",
        "    metrics=['acc']\n",
        ")"
      ],
      "metadata": {
        "id": "8uNPh5-cpC12"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "fiBw7BpMqHWq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(train_data, train_labels, epochs=10, validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lL8zGiPqDyG",
        "outputId": "fa145bd6-7b64-417a-e21f-67e9bafc3e33"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 71s 110ms/step - loss: 0.4441 - acc: 0.7875 - val_loss: 0.3493 - val_acc: 0.8526\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 69s 110ms/step - loss: 0.2071 - acc: 0.9215 - val_loss: 0.3098 - val_acc: 0.8690\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 68s 108ms/step - loss: 0.1071 - acc: 0.9648 - val_loss: 0.3587 - val_acc: 0.8580\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 0.0595 - acc: 0.9805 - val_loss: 0.4277 - val_acc: 0.8714\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 0.0400 - acc: 0.9872 - val_loss: 0.5179 - val_acc: 0.8612\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0226 - acc: 0.9934 - val_loss: 0.5152 - val_acc: 0.8374\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0281 - acc: 0.9914 - val_loss: 0.6126 - val_acc: 0.8448\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0202 - acc: 0.9939 - val_loss: 0.6523 - val_acc: 0.8628\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 68s 109ms/step - loss: 0.0096 - acc: 0.9973 - val_loss: 0.7229 - val_acc: 0.8642\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 67s 108ms/step - loss: 0.0201 - acc: 0.9942 - val_loss: 0.5233 - val_acc: 0.8550\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation"
      ],
      "metadata": {
        "id": "s2Yjt5YQqVCq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(test_data, test_labels)\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTQKNqouqTbP",
        "outputId": "403d7b72-e55f-4b47-e7e2-2904b59ad664"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 16s 21ms/step - loss: 0.5952 - acc: 0.8380\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.5951875448226929, 0.8380399942398071]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Making Predictions\n",
        "Now letâ€™s use our network to make predictions on our own reviews. \n",
        "\n",
        "Since our reviews are encoded well need to convert any review that we write into that form so the network can understand it. To do that well load the encodings from the dataset and use them to encode our own data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D5h0e-YGmQ-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_index = imdb.get_word_index()\n",
        "\n",
        "def encode(text):\n",
        "  token = tf.keras.preprocessing.text.text_to_word_sequence(text)\n",
        "  token = [word_index[word] if word in word_index else 0 for word in token]\n",
        "  return sequence.pad_sequences([token], MAXLEN)[0]\n",
        "\n",
        "\n",
        "reverse_word_index = { value:key for key, value in word_index.items()}\n",
        "\n",
        "def decode(sequence):\n",
        "  PAD = 0\n",
        "  text = ''\n",
        "  for num in sequence:\n",
        "    if num != PAD:\n",
        "      text += reverse_word_index[num] + \" \"\n",
        "  return text[:-1]"
      ],
      "metadata": {
        "id": "TAm52apAvLOA"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Testing the Encoding Function"
      ],
      "metadata": {
        "id": "oYbCV0-LwTCF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pos_review = 'The new Spider Man movie was really amazing, I loved the special effects and the amazing experience.'\n",
        "\n",
        "e = encode(pos_review)\n",
        "\n",
        "print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18W0uq1CwQGR",
        "outputId": "e24cc9b6-32d4-4226-dede-2e938ba19adb"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    1  159 5069  129   17\n",
            "   13   63  477   10  444    1  315  299    2    1  477  582]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(e))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0QRwAnRxBbk",
        "outputId": "be3c1d22-7a5a-48d7-b7c8-be08d5ecab9a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the new spider man movie was really amazing i loved the special effects and the amazing experience\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####Make the prediction"
      ],
      "metadata": {
        "id": "_iEDOYHnyk-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(text):\n",
        "  encoded_text = encode(text)\n",
        "  pred = np.zeros((1,250))\n",
        "  pred[0] = encoded_text\n",
        "  result = model.predict(pred) \n",
        "  print(result[0])"
      ],
      "metadata": {
        "id": "R-iNwI_wyIXz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "positive_review = \"That movie was! really loved it and would great watch it again because it was amazingly great\"\n",
        "predict(positive_review)\n",
        "\n",
        "negative_review = \"that movie really sucked. I hated it and wouldn't watch it again. Was one of the worst things I've ever watched\"\n",
        "predict(negative_review)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bNhbN0Jwwp1_",
        "outputId": "9b9ccbb8-a45c-46c8-c7bf-9efb80273bc7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.927183]\n",
            "[0.24942583]\n"
          ]
        }
      ]
    }
  ]
}